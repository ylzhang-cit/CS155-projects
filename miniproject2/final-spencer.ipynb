{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import nltk\n",
    "import string\n",
    "import pronouncing\n",
    "import numpy as np\n",
    "from HMM import unsupervised_HMM\n",
    "import nltk\n",
    "import string\n",
    "import pronouncing\n",
    "import numpy as np\n",
    "import random\n",
    "with open('./project2data/shakespeare.txt', 'r') as file1: \n",
    "    poems_elements = file1.read().splitlines() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to prove that the stress output is consistent with the syllable of each word\n",
    "for i in range(len(total_words_list)):\n",
    "    if len(stress(i)) != total_words_syllable_list[i]:\n",
    "        print('Error!: the stress and syllable do not match!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#use \n",
    "#make sure 10 syllables in total\n",
    "#make sure the sentence follow unstressed stress pattern\n",
    "def generate_sequence(last_word_index, A_matrix, O_matrix, word_list, word_syllable_list):\n",
    "    sequence_logprob = 0\n",
    "    syllable = word_syllable_list[last_word_index]\n",
    "    # the stress that the previous word should end with\n",
    "    end_stress = (int(stress(last_word_index)[0])+1)%2\n",
    "    L = len(A_matrix)\n",
    "    D = len(O_matrix[0])\n",
    "    #HMM current state\n",
    "    prob = O_matrix[:,last_word_index]/np.sum(O_matrix[:,last_word_index])\n",
    "    Y_curr = np.random.choice(np.arange(L), p = prob)\n",
    "    \n",
    "    #the probability of getting Y for first position\n",
    "    sequence_logprob = sequence_logprob + np.log(prob[Y_curr])\n",
    "    #HMM next state\n",
    "    Y_prev = None\n",
    "    # the index of current X in word_list\n",
    "    X_curr = None\n",
    "    # the output sequence of index in word list \n",
    "    numerical_emission = [last_word_index]\n",
    "    # the output sequence of word\n",
    "    word_emission = [word_list[last_word_index]]\n",
    "    while syllable < 10:\n",
    "        Y_prev = np.random.choice(np.arange(L), p = A_matrix[Y_curr])\n",
    "        sequence_logprob = sequence_logprob +  np.log(A_matrix[Y_curr][Y_prev])\n",
    "        Y_curr = Y_prev\n",
    "        possible_X_curr = possible_word_index_with_end_stress(end_stress)\n",
    "        X_curr = np.random.choice(possible_X_curr, p = O_matrix[Y_curr, possible_X_curr]/np.sum(O_matrix[Y_curr, possible_X_curr]))\n",
    "        sequence_logprob = sequence_logprob + np.log(O_matrix[Y_curr, X_curr])\n",
    "        numerical_emission.insert(0,X_curr)\n",
    "        word_emission.insert(0,word_list[X_curr])\n",
    "        syllable += word_syllable_list[X_curr]\n",
    "        end_stress = (int(stress(X_curr)[0])+1)%2\n",
    "    if syllable == 10:\n",
    "        return word_emission, numerical_emission, sequence_logprob\n",
    "    elif syllable > 10:   \n",
    "        numerical_emission.pop(0)\n",
    "        word_emission.pop(0)\n",
    "        sequence_logprob = sequence_logprob - np.log(O_matrix[Y_curr, X_curr])\n",
    "        syllable -= word_syllable_list[X_curr]\n",
    "        end_stress = (int(stress(numerical_emission[0])[0])+1)%2\n",
    "        left_list_syllable = [i for i, x in enumerate(total_words_syllable_list) if x == 10-syllable]\n",
    "        left_list_stress = possible_word_index_with_end_stress(end_stress)\n",
    "        left_list = np.sort(list(set(left_list_syllable).intersection(left_list_stress)))\n",
    "        X_curr = np.random.choice(left_list, p = O_matrix[Y_curr, left_list]/np.sum(O_matrix[Y_curr, left_list]))\n",
    "        numerical_emission.insert(0,X_curr)\n",
    "        word_emission.insert(0,word_list[X_curr])\n",
    "        sequence_logprob = sequence_logprob + np.log(O_matrix[Y_curr, X_curr])\n",
    "        return word_emission, numerical_emission, sequence_logprob\n",
    "    \n",
    "    return 0\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def readSonnet(fileName):\n",
    "    \"\"\"\n",
    "    This function reads the sonnets, lines, words in the given file.\n",
    "    Input:\n",
    "        Each sonnet has 17 lines: number line, 14 lines, 2 empty lines\n",
    "    Output:\n",
    "        sonnet_lst: the list of sonnet\n",
    "        line_lst: the list of lines in all sonnets\n",
    "        word_set: the set of words appears in the sonnets\n",
    "        last_wd_set: the set of last word in each line\n",
    "    \"\"\"\n",
    "    with open(fileName, 'r') as f:\n",
    "        lines = f.read().splitlines()\n",
    "    sonnet_lst = []\n",
    "    line_lst = []\n",
    "    word_set = set()\n",
    "    last_wd_set = set()\n",
    "    for i, line in enumerate(lines):\n",
    "        j = i % 17\n",
    "        if j == 0:\n",
    "            sonnet = []\n",
    "        elif (j >= 2) and (j <= 15):\n",
    "            sonnet.append(line)\n",
    "            for punc in \"(.,?!'\\\":;)\":\n",
    "                line = line.replace(punc, '')\n",
    "            line = line.replace('-', ' ')\n",
    "            words = nltk.word_tokenize(line.lower())\n",
    "            # words = line.lower().split()\n",
    "            l = []\n",
    "            for word in words:\n",
    "                # add valid word with positive syllables, ignore words with 0 syllable\n",
    "                if stress1(word) != []:\n",
    "                    l.append(word)\n",
    "                    word_set.add(word)\n",
    "            line_lst.append(l)\n",
    "            # add valid last word into the list of last_wd_lst\n",
    "            if stress1(words[-1]) != []:\n",
    "                last_wd_set.add(words[-1]) \n",
    "        elif j == 15:\n",
    "            sonnet_lst.append(sonnet)\n",
    "    return sonnet_lst, line_lst, word_set, last_wd_set\n",
    "\n",
    "def stress1(word):\n",
    "    '''This function returns the list of the given word's stress.'''\n",
    "    result = []\n",
    "    pron = pronouncing.phones_for_word(word)\n",
    "    if pron != []:\n",
    "        for char in pron[0]:\n",
    "            if char.isdigit():\n",
    "                result.append(char)\n",
    "    return result\n",
    "\n",
    "def wd2num(line_lst, word_map):\n",
    "    '''\n",
    "    This function maps a list of sentences to a list of numbers,\n",
    "    and reverses each list of numbers.'''\n",
    "    nums_lst = []\n",
    "    for line in line_lst:\n",
    "        nums = []\n",
    "        for wd in line:\n",
    "            nums.append(word_map[wd])\n",
    "        nums.reverse()\n",
    "        nums_lst.append(nums)\n",
    "    return nums_lst\n",
    "\n",
    "def num2wd(nums_lst, word_lst):\n",
    "    '''\n",
    "    This function maps a list of numbers to a list of sentences,\n",
    "    and reverses each list of sentences.\n",
    "    '''\n",
    "    line_lst = []\n",
    "    for nums in nums_lst:\n",
    "        line = []\n",
    "        for num in nums:\n",
    "            line.append(word_lst[num])\n",
    "        word_lst.reverse()\n",
    "        line_lst.append(line)\n",
    "    return line_lst\n",
    "\n",
    "def HMM():\n",
    "    start = time.time()\n",
    "    sonnet_lst, line_lst, word_set, last_wd_set = readSonnet('./project2data/spenser.txt')\n",
    "    word_lst = list(word_set)\n",
    "    last_wd_lst = list(last_wd_set)\n",
    "    print('The number of sonnets is ', len(sonnet_lst))\n",
    "    print('The number of lines is ', len(line_lst))\n",
    "    print('The number of words is ', len(word_lst))\n",
    "    print('The number of last words is ', len(last_wd_lst))\n",
    "    print('\\n\\n')\n",
    "    # word_lst[i] is the i-th word, word_map[the i-th word] is i\n",
    "    # stress_lst[i] is the stress of the i-th word\n",
    "    # syllab_lst[i] is the number of syllables of the i-th word\n",
    "    word_map = {}\n",
    "    stress_lst = []\n",
    "    syllab_lst = []\n",
    "    for i, wd in enumerate(word_lst):\n",
    "        word_map[wd] = i\n",
    "        wd_stress = stress1(wd)\n",
    "        stress_lst.append(wd_stress)\n",
    "        syllab_lst.append(len(wd_stress))\n",
    "\n",
    "    n_states = 10\n",
    "    n_iters = 100\n",
    "    nums_lst = wd2num(line_lst, word_map)\n",
    "\n",
    "\n",
    "    HMM = unsupervised_HMM(nums_lst, n_states, n_iters)\n",
    "    # Print the transition matrix.\n",
    "    print(\"Transition Matrix:\")\n",
    "    print('#' * 70)\n",
    "    for i in range(len(HMM.A)):\n",
    "        print(''.join(\"{:<12.3e}\".format(HMM.A[i][j]) for j in range(len(HMM.A[i]))))\n",
    "    return (HMM.A, HMM.O)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of sonnets is  0\n",
      "The number of lines is  1232\n",
      "The number of words is  1919\n",
      "The number of last words is  642\n",
      "\n",
      "\n",
      "\n",
      "Transition Matrix:\n",
      "######################################################################\n",
      "4.257e-18   1.451e-01   3.975e-153  3.875e-20   5.258e-16   1.078e-02   8.176e-40   3.775e-01   4.666e-01   3.384e-21   \n",
      "1.159e-27   1.943e-15   1.664e-95   3.249e-21   5.692e-02   2.466e-01   5.048e-01   1.917e-01   1.127e-11   1.499e-22   \n",
      "7.999e-01   4.094e-40   6.848e-231  2.334e-63   9.619e-37   2.001e-01   1.851e-82   2.520e-41   2.630e-56   7.818e-51   \n",
      "9.553e-31   6.971e-02   1.372e-67   1.850e-01   3.551e-20   6.219e-46   4.706e-16   5.608e-19   1.380e-06   7.453e-01   \n",
      "1.948e-29   2.172e-02   6.103e-140  7.512e-01   8.571e-14   1.292e-24   2.197e-45   2.245e-01   2.577e-03   4.586e-20   \n",
      "2.265e-44   2.159e-02   1.373e-139  9.784e-01   7.318e-40   9.820e-53   2.193e-57   1.087e-28   5.711e-43   1.116e-31   \n",
      "7.321e-01   2.127e-12   2.073e-120  8.052e-34   1.563e-35   8.547e-62   2.679e-01   7.919e-33   1.505e-29   2.289e-30   \n",
      "3.757e-30   5.098e-01   7.377e-118  9.280e-03   4.809e-01   1.180e-44   1.382e-38   5.934e-16   2.299e-07   2.976e-21   \n",
      "1.383e-26   1.144e-12   1.259e-160  1.171e-19   4.434e-29   1.749e-48   2.350e-42   9.971e-01   2.916e-03   3.324e-41   \n",
      "9.834e-03   3.446e-02   1.254e-121  7.656e-01   1.140e-02   4.692e-23   2.948e-38   1.460e-01   3.272e-02   1.737e-13   \n"
     ]
    }
   ],
   "source": [
    "HMM_learnedA, HMM_learnedO = HMM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sonnet_lst, line_lst, word_set, total_last_word_set = readSonnet('./project2data/spenser.txt')\n",
    "total_words_list = list(word_set)\n",
    "total_words_syllable_list = []\n",
    "for word in total_words_list:\n",
    "    pronunciation_list = pronouncing.phones_for_word(word)\n",
    "    total_words_syllable_list.append(pronouncing.syllable_count(pronunciation_list[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# return a list of word's stress given the pronounciation of the word\n",
    "def stress_pronoun(pron):\n",
    "    return [char for phone in pron for char in phone if char.isdigit()]\n",
    "# return a list of word's stress given the index of the word in the list\n",
    "def stress(word_index):   \n",
    "    return stress_pronoun(pronouncing.phones_for_word(total_words_list[word_index])[0])\n",
    "\n",
    "word_index_end_1 = []\n",
    "for i in range(len(total_words_list)):\n",
    "    if int(stress(i)[-1]) == 1:\n",
    "        word_index_end_1.append(i)\n",
    "\n",
    "\n",
    "word_index_end_0 = []\n",
    "for i in range(len(total_words_list)):\n",
    "    if int(stress(i)[-1]) == 0:\n",
    "        word_index_end_0.append(i)\n",
    "\n",
    "def possible_word_index_with_end_stress(end_stress_in_int):\n",
    "    if end_stress_in_int == 1:\n",
    "        return word_index_end_1\n",
    "    elif end_stress_in_int == 0:\n",
    "        return word_index_end_0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# return the rhyme of a word\n",
    "def rhyme_of_word(input_word):\n",
    "    pronunciation_list = pronouncing.phones_for_word(input_word)\n",
    "    if pronunciation_list != [] and stress_pronoun(pronunciation_list[0])[-1] == '1':\n",
    "        return pronunciation_list[0].split()[-1]\n",
    "    elif pronunciation_list == []:\n",
    "        return None\n",
    "    \n",
    "# return all the other words in the list that have the same rhyme\n",
    "def words_same_rhyme(input_word, word_list):\n",
    "    rhyme = rhyme_of_word(input_word)\n",
    "    return words_of_rhyme(rhyme, word_list)\n",
    "\n",
    "# return the word that \n",
    "def words_of_rhyme(input_rhyme, word_list):\n",
    "    rhyme = input_rhyme\n",
    "    word_set = set()\n",
    "    for word in word_list:\n",
    "        if rhyme_of_word(word) == rhyme and stress_pronoun(pronouncing.phones_for_word(word)[0])[-1] == '1':\n",
    "            word_set.add(word)\n",
    "    return word_set\n",
    "\n",
    "def fourteen_end_words(word_list):\n",
    "    total_rhyme_set = set()\n",
    "    for word in word_list:\n",
    "        if rhyme_of_word(word) != None:  \n",
    "            total_rhyme_set.add(rhyme_of_word(word))\n",
    "    seven_rhyme_list = list(random.sample(total_rhyme_set, 7))                                      \n",
    "    end_words = [None]*14\n",
    "    for i in range(3):\n",
    "        for j in range(2):\n",
    "            two_words = random.sample(words_of_rhyme(seven_rhyme_list[i*2+j], word_list),2)\n",
    "            end_words[i*4+j] = two_words[0]\n",
    "            end_words[i*4+j+2] = two_words[1]\n",
    "    two_words = random.sample(words_of_rhyme(seven_rhyme_list[0], word_list),2)\n",
    "    end_words[12] = two_words[0]\n",
    "    end_words[13] = two_words[1]\n",
    "    return end_words \n",
    "\n",
    "rhyme_words_list = fourteen_end_words(set(total_words_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "rhyme_words_index_list = []\n",
    "for word in rhyme_words_list:\n",
    "    rhyme_words_index_list.append(total_words_list.index(word))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stress(total_words_list.index('day'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in much and what and all the thoughts degree\n",
      "and goodly flowers happy pleasing ease\n",
      "a captured hungry clatter damsel be\n",
      "a me a that the one the late amaze\n",
      "the gentle laugh in weary way the whit\n",
      "the hope and council dresses happy bough\n",
      "the rolling sorrow that and malice late\n",
      "and great depending catching so the vow\n",
      "the placed in unto were and too and were\n",
      "in likened greater goodly my contain\n",
      "a being do depending cruel her\n",
      "and many sorrow weather lovers moan\n",
      "and being it and dreary sorrows see\n",
      "in iron makers art in greece the thee\n"
     ]
    }
   ],
   "source": [
    "rhyme_words_list = fourteen_end_words(total_last_word_set)\n",
    "rhyme_words_index_list = []\n",
    "for word in rhyme_words_list:\n",
    "    rhyme_words_index_list.append(total_words_list.index(word))\n",
    "\n",
    "for i in rhyme_words_index_list:\n",
    "    x,y,p = generate_sequence(i, A_matrix=np.array(HMM_learnedA), O_matrix=np.array(HMM_learnedO), \n",
    "                  word_list=total_words_list, word_syllable_list = total_words_syllable_list)\n",
    "    print(' '.join(x))\n",
    "    \"\"\"stress_parttern = []\n",
    "    for j in y:\n",
    "        stress_parttern.append(str(stress(j)))\n",
    "    print(''.join(stress_parttern))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for state 0, top 10 words are:\n",
      "is,me,do,will,did,with,a,his,be,\n",
      "\n",
      "for state 1, top 10 words are:\n",
      "the,for,but,on,to,in,with,is,that,\n",
      "\n",
      "for state 2, top 10 words are:\n",
      "prey,store,art,delight,see,appear,sprite,toil,fire,\n",
      "\n",
      "for state 3, top 10 words are:\n",
      "for,but,she,so,i,in,the,to,her,\n",
      "\n",
      "for state 4, top 10 words are:\n",
      "do,fair,will,when,heart,make,which,that,shall,\n",
      "\n",
      "for state 5, top 10 words are:\n",
      "presence,vain,lovely,it,view,now,mind,look,die,\n",
      "\n",
      "for state 6, top 10 words are:\n",
      "for,vain,like,such,but,forth,out,by,sweet,\n",
      "\n",
      "for state 7, top 10 words are:\n",
      "the,their,with,to,that,and,doth,her,i,\n",
      "\n",
      "for state 8, top 10 words are:\n",
      "all,or,a,so,might,heart,love,the,with,\n",
      "\n",
      "for state 9, top 10 words are:\n",
      "eyes,when,be,as,let,more,all,with,that,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    \n",
    "    top10words = np.argsort(HMM_learnedO[i])[-10:-1]\n",
    "    print('for state {}, top 10 words are:'.format(i))\n",
    "    for index in top10words:       \n",
    "        print(total_words_list[index], end=',')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from HMM import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "matrix = np.array(HMM_learnedA)\n",
    "size = matrix[0].size\n",
    "\n",
    "\n",
    "# Create a graph\n",
    "DG = nx.DiGraph()\n",
    "DG.add_nodes_from(range(size))\n",
    "\n",
    "# Add edges and their weights\n",
    "for i in range(size):\n",
    "    for j in range(size):\n",
    "        weight = matrix.item((i, j))\n",
    "        if np.abs(weight) >= 0.05 :\n",
    "            DG.add_edge(i, j, weight=matrix.item((i, j)))\n",
    "\n",
    "# Draw the graph\n",
    "pos = nx.circular_layout(DG)\n",
    "nx.draw(DG, pos, with_labels=True)\n",
    "edge_labels=dict([((u, v, ), '%.2f'%(d['weight'])) for u, v, d in DG.edges(data=True)])\n",
    "nx.draw_networkx_edge_labels(DG, pos, edge_labels=edge_labels, label_pos=0.27, font_size=7)\n",
    "plt.axis('off')\n",
    "plt.savefig('spencer.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
