{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "'''Example script to generate shakespeare sonnets.\n",
    "At least 20 epochs are required before the generated text\n",
    "starts sounding coherent.\n",
    "'''\n",
    "\n",
    "\n",
    "from __future__ import print_function\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.recurrent import LSTM\n",
    "#from keras.datasets.data_utils import get_file\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "import nltk as nltk\n",
    "\n",
    "def readSonnet(fileName):\n",
    "    \"\"\"\n",
    "    This function reads the sonnets, lines, words in the given file.\n",
    "    Input:\n",
    "        Each sonnet has 17 lines: number line, 14 lines, 2 empty lines\n",
    "    Output:\n",
    "        sonnet_lst: the list of sonnet\n",
    "        line_lst: the list of lines in all sonnets\n",
    "        word_dict: the dictionary of words appears in the sonnets\n",
    "    \"\"\"\n",
    "    with open(fileName, 'r') as f:\n",
    "        lines = f.read().splitlines()\n",
    "    sonnet_lst = []\n",
    "    line_lst = []\n",
    "    word_dict = dict()\n",
    "    word_set = set()\n",
    "    for i, line in enumerate(lines):\n",
    "        j = i % 17\n",
    "        if j == 0:\n",
    "            sonnet = []\n",
    "        elif (j >= 1) and (j <= 14):\n",
    "            for punc in \",.?:;!()\":\n",
    "                line = line.replace(punc, ' ').lower()\n",
    "            sonnet.append(line)\n",
    "            line_lst.append(line)\n",
    "            words = nltk.tokenize.word_tokenize(line)\n",
    "            for word in words:\n",
    "                word = word.lower()\n",
    "                try:\n",
    "                    word_dict[word] += 1\n",
    "                    word_set.add(word)\n",
    "                except KeyError:\n",
    "                    word_dict[word] = 1\n",
    "                    word_set.add(word)\n",
    "\n",
    "        elif j == 15:\n",
    "            sonnet_lst.append(sonnet)\n",
    "    return sonnet_lst, line_lst, word_dict, word_set\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of sonnets is  152\n",
      "The number of lines is  2128\n",
      "The number of words is  3122\n",
      "total chars: 3122\n",
      "nb sequences: 2127\n"
     ]
    }
   ],
   "source": [
    "sonnet_lst, line_lst, word_dict, word_set = readSonnet('shakespeare.txt')\n",
    "print('The number of sonnets is ', len(sonnet_lst))\n",
    "print('The number of lines is ', len(line_lst))\n",
    "print('The number of words is ', len(word_dict))\n",
    "\n",
    "#text = open('skp_clean.txt').read().lower()\n",
    "#print ('corpus length:', len(text))\n",
    "#print (text)\n",
    "\n",
    "word = list(word_set)\n",
    "print('total chars:', len(word))\n",
    "#print (word)\n",
    "\n",
    "word_indices = dict((c, i) for i, c in enumerate(word))#construct a dictionary to make it easy to vectorize the input text...\n",
    "indices_word = dict((i, c) for i, c in enumerate(word))\n",
    "\n",
    "#pickle.dump(char_indices, open(\"char_indic.json\", \"wb\"))#file format control\n",
    "#pickle.dump(indices_char, open(\"indic_char.json\", \"wb\"))\n",
    "\n",
    "\n",
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "\n",
    "maxlen = 0\n",
    "step = 1\n",
    "sentences = []\n",
    "next_sentence = []\n",
    "for i in range(0, len(line_lst)-1, step):\n",
    "    sentence_words =nltk.tokenize.word_tokenize(line_lst[i])\n",
    "    sentences.append(sentence_words)\n",
    "    if len(sentence_words) >maxlen:\n",
    "        maxlen = len(sentence_words)\n",
    "    sentence_words =nltk.tokenize.word_tokenize(line_lst[i+1])\n",
    "    next_sentence.append(sentence_words)\n",
    "print('nb sequences:', len(sentences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization...\n",
      "2127\n",
      "12\n",
      "Build model...\n",
      "12 3122\n"
     ]
    }
   ],
   "source": [
    "#print (sentences[0:10])\n",
    "#print (next_sentence[0:10])\n",
    "import time\n",
    "start = time.time()\n",
    "print('Vectorization...')\n",
    "X = np.zeros((len(sentences), maxlen, len(word)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), maxlen, len(word)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    if i<len(sentences) - 1:\n",
    "        sentence_next = sentences[i+1]\n",
    "        for t, word_t in enumerate(sentence):\n",
    "            X[i, t, word_indices[word_t]] = 1\n",
    "        for t, word_t in enumerate(sentence_next):\n",
    "            y[i, t, word_indices[word_t]] = 1\n",
    "print (len(sentences))\n",
    "print (maxlen)\n",
    "\n",
    "\n",
    "# build the model: 2 stacked LSTM\n",
    "print('Build model...')\n",
    "print (maxlen, len(word))\n",
    "model = Sequential()\n",
    "#model.add(LSTM(512, return_sequences = True, input_dim=(maxlen,len(word)), input_length=len(sentences)))\n",
    "model.add(LSTM(512, return_sequences=True, input_shape=(maxlen, len(word))))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(512, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(len(word)))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "\n",
    "def sample(a, temperature):\n",
    "    a = np.exp(a)\n",
    "    if np.sum(a)==0:\n",
    "        return (random.randint(0, len(word)-2) )\n",
    "    else:\n",
    "        a /= np.sum(a)\n",
    "    return np.argmax(np.random.multinomial(1, a, 1))\n",
    "\n",
    "\n",
    "#def sample(a, temperature):\n",
    "    # helper function to sample an index from a probability array\n",
    "    #a = np.log(a) / temperature\n",
    "    #a = np.exp(a) / np.sum(np.exp(a))\n",
    "    \n",
    "#    tem = np.sum(a[0:len(a)-1]);\n",
    "#    b = a[0:len(a)-1]\n",
    "#    b = np.hstack((b, tem))\n",
    "    #print(a)\n",
    "    #b = a\n",
    "#    return np.argmax(np.random.multinomial(1, b, 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------\n",
      "Iteration 1\n",
      "Epoch 1/1\n",
      "2127/2127 [==============================] - 54s - loss: 3.9758     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \" ['o', 'but', 'with', 'mine', 'compare', 'thou', 'thine', 'own', 'state'] \"\n",
      " pity thrall else special rest fixed twenty unfair \n",
      "\n",
      " deceivest building whole spent princes subsist self-love fearing \n",
      "\n",
      " picture hand reeks loan pry unbred time-bettering mars \n",
      "\n",
      " steep-up limits fears seem o'ertake thrust ruinate sober \n",
      "\n",
      " trial gilded forbidden grecian wilfully become then intelligence \n",
      "\n",
      " exchequer mourning widow towers beweep needs decree misuse \n",
      "\n",
      " scandal fester contains succeeding cover given stained blind \n",
      "\n",
      " warmed flattery full ill-used paying misuse enemies pardon \n",
      "\n",
      " sober league pitch seen sees maiden air locked \n",
      "\n",
      " dreading increase tempting vial paying shaken grace strongly \n",
      "\n",
      " blood filed looks doth precious painful unbless striving \n",
      "\n",
      " separable enlarged children hated be addition gain last \n",
      "\n",
      " couldst so besides pitying made weakens pity lascivious \n",
      "\n",
      " given age proceed beweep torment hour measure up-locked \n",
      "\n",
      "\n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \" ['o', 'but', 'with', 'mine', 'compare', 'thou', 'thine', 'own', 'state'] \"\n",
      " slide verses afford abide who preserve banquet rents \n",
      "\n",
      " unhappily begins stirred turned five titles plants civil \n",
      "\n",
      " proclaims interest paying felt saint love-suit hard fairly \n",
      "\n",
      " prognosticate thunder word gates appearance absence concord pretty \n",
      "\n",
      " allow fall teachest pardon brood doubting story sums \n",
      "\n",
      " sequent steal concealed course lip lends intermixed rolling \n",
      "\n",
      " hearing blessed tincture grace checked noon never-resting featureless \n",
      "\n",
      " salutation watching climbed thrusts else possesseth same fiery \n",
      "\n",
      " stole defy lesson pitying body hang sour inhabit \n",
      "\n",
      " angel grave pardon miscalled nobler despair things humble \n",
      "\n",
      " drink looked not hairs process kingdom sorrow despite \n",
      "\n",
      " o'erlook pursuit transport effectually brand end contents diseased \n",
      "\n",
      " erred woe discloses testy knights dwell approve former \n",
      "\n",
      " wakened decree beard siren duty bloody sickle need'st \n",
      "\n",
      "\n",
      "the total running time is 66.7213089466095\n"
     ]
    }
   ],
   "source": [
    "# train the model, output generated text after each iteration\n",
    "#for iteration in range(1, 60):\n",
    "for iteration in range(1, 2):\n",
    "    print()\n",
    "    print('-' * 50)\n",
    "    print('Iteration', iteration)\n",
    "    model.load_weights(\"weights.h5\")\n",
    "    model.fit(X, y, batch_size=128, nb_epoch=1)\n",
    "\n",
    "    # dump the model to disk\n",
    "    json_string = model.to_json()\n",
    "    open(\"arch.json\", \"w\").write(json_string)\n",
    "    model.save_weights(\"weights.h5\", overwrite=True)\n",
    "\n",
    "start_index = random.randint(0, len(line_lst) - maxlen - 1)\n",
    "\n",
    "for diversity in [0.2,0.5]:\n",
    "    print()\n",
    "    print('----- diversity:', diversity)\n",
    "\n",
    "    generated = []\n",
    "    sentence = sentences[start_index]\n",
    "    generated.append (sentence)\n",
    "    print('----- Generating with seed: \"', sentence, '\"')\n",
    "    #sys.stdout.write(generated)\n",
    "    sonnet_len = 14;#length of the sonnet\n",
    "\n",
    "    for i in range(sonnet_len):\n",
    "        x = np.zeros((1, maxlen, len(word)))\n",
    "        for t, word_t in enumerate(sentence):\n",
    "            x[0, t, word_indices[word_t]] = 1.\n",
    "\n",
    "        preds = model.predict(x, verbose=0)[0]\n",
    "        #print (preds)\n",
    "        #print (preds)\n",
    "        sentence_now = []\n",
    "        line_now =''\n",
    "        #for ip in range (maxlen):\n",
    "        for ip in range (8):\n",
    "            prediction = preds[ip]\n",
    "            #print(prediction)\n",
    "            #print(len(prediction))\n",
    "            #print(len(prediction[1]))\n",
    "            next_index = sample(prediction, diversity)#sample function need to be redefined\n",
    "            next_word = indices_word[next_index]\n",
    "            sentence_now.append(next_word);\n",
    "            generated.append(next_word)\n",
    "#             sys.stdout.write(next_word)\n",
    "            line_now = line_now + ' ' + next_word\n",
    "#               print (next_word)\n",
    "        sentence = sentence_now\n",
    "        print (line_now,'\\n')\n",
    "#           sys.stdout.flush()\n",
    "    print()\n",
    "end = time.time()\n",
    "print ('the total running time is', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.69314718,  1.09861229,  1.38629436])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----- diversity: 0.2\n",
    "----- Generating with seed: \" ['that', 'tongue', 'that', 'tells', 'the', 'story', 'of', 'thy', 'days'] \"\n",
    "\n",
    " that can live yet runs this unperfect things \n",
    "\n",
    "\n",
    " like stones see delivers vice disdaineth true did \n",
    "\n",
    "\n",
    " but life progress day hath masonry restore if \n",
    "\n",
    "\n",
    " i mine for now subject ear give afford \n",
    "\n",
    "\n",
    " though end seasons that prescriptions worthy life ' \n",
    "\n",
    "\n",
    " which proudest that who confound wrackful see lambs \n",
    "\n",
    " can see small more blood not writ disposed \n",
    "\n",
    "\n",
    " is o'ercharged argument him frown'st theirs virgin blesses \n",
    "\n",
    "\n",
    " for defaced virtuous dignified 's will fire under \n",
    "\n",
    " and want and needs have be of pleasing \n",
    "\n",
    "\n",
    " to present-absent balmy stick'st come hidden verse defendant \n",
    "\n",
    " vulgar love unhappily well with time slave death \n",
    "\n",
    "\n",
    " my harmful ashes and two brain dark ' \n",
    "\n",
    "\n",
    " to heaven self thee me me the society "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
