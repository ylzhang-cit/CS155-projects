{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.78318 (+/- 0.0004141707754557466536482) [GradientBoost]\n",
      "Accuracy: 0.78173 (+/- 0.0004208989450147915256650) [XGB]\n",
      "Accuracy: 0.78264 (+/- 0.0001822048386114216711462) [Ensemble]\n",
      "[14098, 1902]\n",
      "[72648, 10172]\n",
      "The running time is  924.8110620975494\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn import linear_model\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "def normalize(x):\n",
    "    '''This function nomalizes each columns of the input 2d array.'''\n",
    "    x_mean = np.mean(x, axis=0)\n",
    "    x_std = np.std(x, axis=0)\n",
    "    x_std[x_std == 0] = 1\n",
    "    x1 = (x - x_mean) / x_std\n",
    "    return x1\n",
    "\n",
    "def addFeature (X_train, X_test1, X_test2, add_feature):\n",
    "    addFeatureLen = len(add_feature)\n",
    "    for i1 in range (addFeatureLen):\n",
    "        for i2 in range (i1,addFeatureLen, 1):\n",
    "            newFeature1 = np.array(X_train[:,i1]*X_train[:,i2]).reshape(-1,1)\n",
    "            #print (np.shape(newFeature1))\n",
    "            X_train = np.hstack((X_train, newFeature1))\n",
    "            newFeature2 = np.array(X_test1[:,i1]*X_test1[:,i2]).reshape(-1,1)\n",
    "            X_test1 = np.hstack((X_test1, newFeature2))\n",
    "            newFeature3 = np.array(X_test2[:,i1]*X_test2[:,i2]).reshape(-1,1)\n",
    "            X_test2 = np.hstack((X_test2, newFeature3))\n",
    "    return (X_train, X_test1, X_test2)\n",
    "\n",
    "def selectFeature(X_train, y_train, x_test1, x_test2,alpha1):\n",
    "    '''This function select the features of normalized data (i.e., np.std(X[:,j]) = 1 or 0).\n",
    "    If qmin < (np.amax(X[:,j]) - np.amin(X[:,j]) < qmax, then j will be selected.'''\n",
    "    reg = linear_model.Lasso(alpha = alpha1)\n",
    "    reg.fit(X_train, y_train)\n",
    "    keeplist = []\n",
    "    feature_num = len(reg.coef_)\n",
    "    for i in range(feature_num):\n",
    "        if abs(reg.coef_[i])> 1e-4:\n",
    "            keeplist.append(True)\n",
    "        else:\n",
    "            keeplist.append(False)\n",
    "    #keeplist[0] = True\n",
    "    keeplist = np.array(keeplist)\n",
    "    #print (reg.coef_[[0, 6, 7, 39, 41]])\n",
    "    #for index, value in enumerate(keeplist):\n",
    "    #    if value:\n",
    "    #        print (index)\n",
    "    #print (reg.coef_[52])\n",
    "    #dist = np.amax(X_train, axis=0) - np.amin(X_train, axis=0)\n",
    "    #cols = np.all([dist > qmin, dist < qmax], axis=0)\n",
    "    x_train_new = X_train[:, keeplist]\n",
    "    x_test1_new = X_test1[:, keeplist]\n",
    "    x_test2_new = X_test2[:, keeplist]\n",
    "    return (x_train_new, x_test1_new, x_test2_new)\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# load the the data from the files\n",
    "with open('train_2008.csv', 'r') as file1: \n",
    "    lines1 = csv.reader(file1, delimiter=',', quotechar='|') \n",
    "    next(lines1, None)\n",
    "    data1 = np.array([line for line in lines1], dtype=float)\n",
    "\n",
    "with open('test_2008.csv', 'r') as file2:\n",
    "\tlines2 = csv.reader(file2, delimiter=',', quotechar='\"')\n",
    "\tnext(lines2, None)\n",
    "\tdata2 = np.array([line for line in lines2], dtype=float)\n",
    "\n",
    "with open('test_2012.csv', 'r') as file3:\n",
    "\tlines3 = csv.reader(file3, delimiter=',', quotechar='\"')\n",
    "\tnext(lines3, None)\n",
    "\tdata3 = np.array([line for line in lines3], dtype=float)\n",
    "\n",
    "# convert the data to float numpy array \n",
    "N_train = len(data1)\n",
    "add_feature = [6,11,39,41,45,48,52,57,64,75,200,332,335,337,348,363,371,374,388,406,417,424,426];\n",
    "alpha1 = 0.005\n",
    "y_train = 2 * (data1[:, -1] - 1.5)  # maps 1 to -1, 2 to 1\n",
    "X_train = normalize(data1[:, :-1])\n",
    "X_train[:, 0] = 1\n",
    "X_test1 = normalize(data2)\n",
    "X_test1[:, 0] = 1\n",
    "X_test2 = normalize(data3)\n",
    "X_test2[:, 0] = 1\n",
    "#qmin, qmax = 1, 100\n",
    "X_train, X_test1, X_test2 = addFeature(X_train, X_test1, X_test2, add_feature)\n",
    "X_train, X_test1, X_test2 = selectFeature(X_train, y_train, X_test1, X_test2, alpha1) \n",
    "d = len(X_train[0])\n",
    "\n",
    "\n",
    "# train the model and calculate the scores by cross-validation\n",
    "N = 550\n",
    "clf1 = AdaBoostClassifier(n_estimators=N)\n",
    "clf2 = GradientBoostingClassifier(n_estimators=N)\n",
    "clf3 = RandomForestClassifier(n_estimators=N)\n",
    "clf4 = BaggingClassifier(n_estimators=N)\n",
    "clf5 = xgb.XGBClassifier(max_depth=4, silent = 1, objective = 'binary:logistic')\n",
    "eclf = VotingClassifier(estimators=[ ('gb', clf2),('xgb', clf5)], voting='hard')\n",
    "#eclf = VotingClassifier(estimators=[('ab', clf1), ('gb', clf2), ('rf', clf3), ('bg',clf4)], voting='hard')\n",
    "clf_lst = [clf2, clf5, eclf]\n",
    "name_lst = [ 'GradientBoost', 'XGB', 'Ensemble']\n",
    "for clf, name in zip(clf_lst, name_lst):\n",
    "    scores = cross_val_score(clf, X_train, y_train, cv=2, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.5f (+/- %0.25f) [%s]\" % (scores.mean(), scores.std(), name))\n",
    "\n",
    "# write the prediction data into the submission file\n",
    "eclf.fit(X_train, y_train)\n",
    "y_test1 = eclf.predict(X_test1)\n",
    "print([sum(y_test1==-1), sum(y_test1==1)])\n",
    "with open('submission2008.csv', 'w', newline='') as file: \n",
    "\tfilewriter = csv.writer(file, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "\tfilewriter.writerow(['id', 'PES1'])\n",
    "\tfor i, yi in enumerate(y_test1):\n",
    "\t\tfilewriter.writerow([str(i), str(int(yi/2 + 1.5))])\n",
    "y_test2 = eclf.predict(X_test2)\n",
    "print([sum(y_test2==-1), sum(y_test2==1)])\n",
    "with open('submission2012.csv', 'w', newline='') as file: \n",
    "\tfilewriter = csv.writer(file, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "\tfilewriter.writerow(['id', 'PES1'])\n",
    "\tfor i, yi in enumerate(y_test2):\n",
    "\t\tfilewriter.writerow([str(i), str(int(yi/2 + 1.5))])\n",
    "\n",
    "\n",
    "# print running time\n",
    "stop = time.time()\n",
    "print('The running time is ', stop - start)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "soft:\n",
    "Accuracy: 0.77894 (+/- 0.0013178445819652662152066) [Adaboost]\n",
    "Accuracy: 0.78116 (+/- 0.0004982269819299722790618) [GradientBoost]\n",
    "Accuracy: 0.77653 (+/- 0.0003281962658751025330162) [RandomForest]\n",
    "Accuracy: 0.78235 (+/- 0.0005446000823553509562203) [Ensemble]\n",
    "\n",
    "if add GaussianNB    \n",
    "Accuracy: 0.77894 (+/- 0.0013178445819652662152066) [Adaboost]\n",
    "Accuracy: 0.78122 (+/- 0.0004672983497709393141406) [GradientBoost]\n",
    "Accuracy: 0.77735 (+/- 0.0000274846778660076118683) [RandomForest]\n",
    "Accuracy: 0.73221 (+/- 0.0117019842098985971112768) [Ensemble]\n",
    "\n",
    "if add logit\n",
    "Accuracy: 0.77894 (+/- 0.0013178445819652662152066) [Adaboost]\n",
    "Accuracy: 0.78123 (+/- 0.0004827619484596423760081) [GradientBoost]\n",
    "Accuracy: 0.77582 (+/- 0.0005686953276320960704027) [RandomForest]\n",
    "Accuracy: 0.77375 (+/- 0.0007612267682565732052069) [LogitEnsemble]\n",
    "\n",
    "hard but with 150 nodes\n",
    "Accuracy: 0.77894 (+/- 0.0013178445819652662152066) [Adaboost]\n",
    "Accuracy: 0.78112 (+/- 0.0004672997845526216664780) [GradientBoost]\n",
    "Accuracy: 0.77613 (+/- 0.0000738573000307884974802) [RandomForest]\n",
    "Accuracy: 0.78236 (+/- 0.0004672806541302088056966) [Ensemble]\n",
    "[13890, 2110]\n",
    "[71836, 10984]\n",
    "The running time is  343.1007869243622\n",
    "\n",
    "with 200 nodes with feature selection\n",
    "Accuracy: 0.78023 (+/- 0.0012714332206951173276366) [Adaboost]\n",
    "Accuracy: 0.78179 (+/- 0.0000430171460753525636278) [GradientBoost]\n",
    "Accuracy: 0.77792 (+/- 0.0002353917172360775467155) [RandomForest]\n",
    "Accuracy: 0.78259 (+/- 0.0005136685806329532866243) [Ensemble]\n",
    "\n",
    "with 200 nodes, without feature selection\n",
    "Accuracy: 0.77871 (+/- 0.0004295655046238078256238) [Adaboost]\n",
    "Accuracy: 0.78184 (+/- 0.0000961566769045463232146) [GradientBoost]\n",
    "Accuracy: 0.77386 (+/- 0.0001511413369744696311159) [RandomForest]\n",
    "Accuracy: 0.78173 (+/- 0.0000806945129975256136845) [Ensemble]\n",
    "\n",
    "with 200 nodes, without feature selection; add bagging as the fourth\n",
    "Accuracy: 0.77871 (+/- 0.0004295655046238078256238) [Adaboost]\n",
    "Accuracy: 0.78179 (+/- 0.0001734765833901752429824) [GradientBoost]\n",
    "Accuracy: 0.77398 (+/- 0.0004364825871040478588725) [RandomForest]\n",
    "Accuracy: 0.77676 (+/- 0.0011168484076881646238633) [Bagging]\n",
    "Accuracy: 0.78131 (+/- 0.0005291522662650982589128) [Ensemble]\n",
    "\n",
    "with 200 nodes, with feature selection; add bagging as the fourth\n",
    "Accuracy: 0.78023 (+/- 0.0012714332206951173276366) [Adaboost]\n",
    "Accuracy: 0.78173 (+/- 0.0000739438651921608958162) [GradientBoost]\n",
    "Accuracy: 0.77676 (+/- 0.0000034520847225216755305) [RandomForest]\n",
    "Accuracy: 0.77676 (+/- 0.0012096314346020098362544) [Bagging]\n",
    "Accuracy: 0.78222 (+/- 0.0009466617786906827980431) [Ensemble]\n",
    "\n",
    "Accuracy: 0.78023 (+/- 0.0012714332206951173276366) [Adaboost]\n",
    "Accuracy: 0.78174 (+/- 0.0000275525908655094298183) [GradientBoost]\n",
    "Accuracy: 0.77704 (+/- 0.0002439736247259460810710) [RandomForest]\n",
    "Accuracy: 0.77611 (+/- 0.0002199558576598814596537) [Bagging]\n",
    "Accuracy: 0.78204 (+/- 0.0009466646482541030138691) [Ensemble]\n",
    "[14149, 1851]\n",
    "[73040, 9780]\n",
    "The running time is  1419.6259248256683"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy: 0.78177 (+/- 0.0000275530691261072213649) [GradientBoost]\n",
    "Accuracy: 0.78140 (+/- 0.0005291508314834714177266) [XGB]\n",
    "Accuracy: 0.78289 (+/- 0.0003744894967868672708278) [Ensemble]\n",
    "[13871, 2129]\n",
    "[71712, 11108]\n",
    "The running time is  436.3834500312805\n",
    "\n",
    "Accuracy: 0.78112 (+/- 0.0004363721089146732623476) [GradientBoost]\n",
    "Accuracy: 0.78140 (+/- 0.0005291508314834714177266) [XGB]\n",
    "Accuracy: 0.78148 (+/- 0.0005446134736510344076521) [Ensemble]\n",
    "[14027, 1973]\n",
    "[72592, 10228]\n",
    "The running time is  291.1992840766907\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
