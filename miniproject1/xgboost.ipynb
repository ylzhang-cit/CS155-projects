{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.tree\n",
    "import csv\n",
    "import sklearn.ensemble\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    '''This function nomalizes each columns of the input 2d array.'''\n",
    "    x_mean = np.mean(x, axis=0)\n",
    "    x_std = np.std(x, axis=0)\n",
    "    x_std[x_std == 0] = 1\n",
    "    x1 = (x - x_mean) / x_std\n",
    "    return x1\n",
    "\n",
    "def xgb_test_accuracy(preds_prob, test_label):\n",
    "    preds_label = preds_prob > 0.5\n",
    "    accuracy = 0\n",
    "    for i in range(len(test_label)):\n",
    "        if preds_label[i] == test_label[i]:\n",
    "            accuracy += 1\n",
    "    return accuracy\n",
    "\n",
    "def xgb_feature_testerror(input_train_data, input_test_data, feature_index):\n",
    "    X_train = input_train_data[:,[0,feature_index]]\n",
    "    Y_train = input_train_data[:,382]\n",
    "    Y_train[Y_train == 2.0] = 0\n",
    "    X_test = input_test_data[:,[0,feature_index]]\n",
    "    Y_test = input_test_data[:,382]\n",
    "    Y_test[Y_test == 2.0] = 0\n",
    "    dtrain = xgb.DMatrix(X_train, label=Y_train)\n",
    "    dtest = xgb.DMatrix(X_test, label=Y_test)\n",
    "    param = {'max_depth':4, 'eta':0.1, 'silent':1, 'objective':'binary:logistic' }\n",
    "    num_round = 10\n",
    "    bst = xgb.train(param, dtrain, num_round)\n",
    "    # make prediction\n",
    "    preds = bst.predict(dtest)\n",
    "    test_accuracy = xgb_test_accuracy(preds, Y_test)\n",
    "    return test_accuracy/len(Y_test)\n",
    "\n",
    "def selectFeature(X_train, y_train, x_test1, x_test2):\n",
    "    '''This function select the features of normalized data (i.e., np.std(X[:,j]) = 1 or 0).\n",
    "    If qmin < (np.amax(X[:,j]) - np.amin(X[:,j]) < qmax, then j will be selected.'''\n",
    "    reg = linear_model.Lasso(alpha = 0.002)\n",
    "    reg.fit(X_train, y_train)\n",
    "    keeplist = []\n",
    "    feature_num = len(reg.coef_)\n",
    "    for i in range(feature_num):\n",
    "        if abs(reg.coef_[i])> 1e-4:\n",
    "            keeplist.append(True)\n",
    "        else:\n",
    "            keeplist.append(False)\n",
    "    keeplist = np.array(keeplist)\n",
    "    x_train_new = X_train[:, keeplist]\n",
    "    x_test1_new = X_test1[:, keeplist]\n",
    "    x_test2_new = X_test2[:, keeplist]\n",
    "    return (x_train_new, x_test1_new, x_test2_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152\n"
     ]
    }
   ],
   "source": [
    "# load the the data from the files\n",
    "with open('train_2008.csv', 'r') as file1: \n",
    "    lines1 = csv.reader(file1, delimiter=',', quotechar='|') \n",
    "    next(lines1, None)\n",
    "    data1 = np.array([line for line in lines1], dtype=float)\n",
    "\n",
    "with open('test_2008.csv', 'r') as file2:\n",
    "\tlines2 = csv.reader(file2, delimiter=',', quotechar='\"')\n",
    "\tnext(lines2, None)\n",
    "\tdata2 = np.array([line for line in lines2], dtype=float)\n",
    "\n",
    "with open('test_2012.csv', 'r') as file3:\n",
    "\tlines3 = csv.reader(file3, delimiter=',', quotechar='\"')\n",
    "\tnext(lines3, None)\n",
    "\tdata3 = np.array([line for line in lines3], dtype=float)\n",
    "\n",
    "# convert the data to float numpy array \n",
    "N_train = len(data1)\n",
    "y_train = 2 * (data1[:, -1] - 1.5)  # maps 1 to -1, 2 to 1\n",
    "X_train = normalize(data1[:, :-1])\n",
    "X_train[:, 0] = 1\n",
    "X_test1 = normalize(data2)\n",
    "X_test1[:, 0] = 1\n",
    "X_test2 = normalize(data3)\n",
    "X_test2[:, 0] = 1\n",
    "#qmin, qmax = 1, 100\n",
    "X_train, X_test1, X_test2 = selectFeature(X_train, y_train, X_test1, X_test2) \n",
    "\n",
    "d = len(X_train[0])\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'validation_0': {'error': [0.246737,\n",
       "   0.246954,\n",
       "   0.236408,\n",
       "   0.244665,\n",
       "   0.239407,\n",
       "   0.236871,\n",
       "   0.236995,\n",
       "   0.236593,\n",
       "   0.237057,\n",
       "   0.234274,\n",
       "   0.233315,\n",
       "   0.233377,\n",
       "   0.233036,\n",
       "   0.232325,\n",
       "   0.232511,\n",
       "   0.232573,\n",
       "   0.23217,\n",
       "   0.231366,\n",
       "   0.231459,\n",
       "   0.231521,\n",
       "   0.230006,\n",
       "   0.229387,\n",
       "   0.229047,\n",
       "   0.228552,\n",
       "   0.228552,\n",
       "   0.228985,\n",
       "   0.228181,\n",
       "   0.227377,\n",
       "   0.227408,\n",
       "   0.227067,\n",
       "   0.22648,\n",
       "   0.225892,\n",
       "   0.225676,\n",
       "   0.225336,\n",
       "   0.225274,\n",
       "   0.224408,\n",
       "   0.224253,\n",
       "   0.224253,\n",
       "   0.224098,\n",
       "   0.224253,\n",
       "   0.223882,\n",
       "   0.223418,\n",
       "   0.223233,\n",
       "   0.223171,\n",
       "   0.222985,\n",
       "   0.223078,\n",
       "   0.223109,\n",
       "   0.222521,\n",
       "   0.223016,\n",
       "   0.222676,\n",
       "   0.222985,\n",
       "   0.222583,\n",
       "   0.222336,\n",
       "   0.222645,\n",
       "   0.222428,\n",
       "   0.222645,\n",
       "   0.222243,\n",
       "   0.222181,\n",
       "   0.222336,\n",
       "   0.222428,\n",
       "   0.222088,\n",
       "   0.222243,\n",
       "   0.222212,\n",
       "   0.22215,\n",
       "   0.222119,\n",
       "   0.222397,\n",
       "   0.222428,\n",
       "   0.222676,\n",
       "   0.222119,\n",
       "   0.222459,\n",
       "   0.222181,\n",
       "   0.222119,\n",
       "   0.221964,\n",
       "   0.221903,\n",
       "   0.222026,\n",
       "   0.222088,\n",
       "   0.222026,\n",
       "   0.222119,\n",
       "   0.221934,\n",
       "   0.221748,\n",
       "   0.221686,\n",
       "   0.221748,\n",
       "   0.221995,\n",
       "   0.221934,\n",
       "   0.22215,\n",
       "   0.222212,\n",
       "   0.222181,\n",
       "   0.222088,\n",
       "   0.222181,\n",
       "   0.22215,\n",
       "   0.222367,\n",
       "   0.222367,\n",
       "   0.222336,\n",
       "   0.222305,\n",
       "   0.222057,\n",
       "   0.222026,\n",
       "   0.222057,\n",
       "   0.221903,\n",
       "   0.22215,\n",
       "   0.222088]}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model and calculate the scores by cross-validation\n",
    "\n",
    "clf = xgb.XGBClassifier(max_depth=4, silent = 1, objective = 'binary:logistic')\n",
    "clf.fit(X_train[:int(N_train/2)], y_train[:int(N_train/2)], eval_set=[(X_train[int(N_train/2):], y_train[int(N_train/2):])], eval_metric='error', verbose=False)\n",
    "clf.evals_result_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0041841 ,  0.00209205,  0.00209205,  0.01882845,  0.0097629 ,\n",
       "        0.00488145,  0.00069735,  0.00348675,  0.05439331,  0.01185495,\n",
       "        0.00488145,  0.00906555,  0.0013947 ,  0.00348675,  0.0041841 ,\n",
       "        0.        ,  0.00488145,  0.05439331,  0.03556485,  0.0223152 ,\n",
       "        0.00209205,  0.0055788 ,  0.01046025,  0.01743375,  0.013947  ,\n",
       "        0.00069735,  0.08437936,  0.00069735,  0.00906555,  0.00906555,\n",
       "        0.06345886,  0.05718271,  0.00906555,  0.0013947 ,  0.01185495,\n",
       "        0.        ,  0.        ,  0.        ,  0.0097629 ,  0.02161785,\n",
       "        0.0111576 ,  0.00488145,  0.00069735,  0.03695956,  0.0083682 ,\n",
       "        0.0083682 ,  0.00069735,  0.00906555,  0.00069735,  0.        ,\n",
       "        0.00488145,  0.0041841 ,  0.        ,  0.01046025,  0.0013947 ,\n",
       "        0.00767085,  0.0041841 ,  0.00906555,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.00069735,  0.0013947 ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.0027894 ,  0.00209205,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.0069735 ,  0.0041841 ,  0.        ,  0.00348675,  0.00209205,\n",
       "        0.0041841 ,  0.0013947 ,  0.0027894 ,  0.00488145,  0.        ,\n",
       "        0.        ,  0.        ,  0.00069735,  0.00348675,  0.0027894 ,\n",
       "        0.0125523 ,  0.0055788 ,  0.04811715,  0.02301255,  0.00069735,\n",
       "        0.        ,  0.        ,  0.0027894 ,  0.        ,  0.00069735,\n",
       "        0.00348675,  0.        ,  0.0027894 ,  0.0013947 ,  0.        ,\n",
       "        0.        ,  0.        ,  0.0027894 ,  0.        ,  0.        ,\n",
       "        0.00069735,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.00069735,  0.        ,\n",
       "        0.00069735,  0.00069735,  0.        ,  0.        ,  0.0041841 ,\n",
       "        0.00209205,  0.00767085,  0.0041841 ,  0.00209205,  0.        ,\n",
       "        0.00069735,  0.        ,  0.0027894 ,  0.        ,  0.        ,\n",
       "        0.0404463 ,  0.03138075,  0.        ,  0.        ,  0.0069735 ,\n",
       "        0.0041841 ,  0.        ,  0.0083682 ,  0.0055788 ,  0.00348675,\n",
       "        0.00069735,  0.00069735,  0.0055788 ,  0.0027894 ,  0.0027894 ,\n",
       "        0.0097629 ,  0.        ], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 8, 17, 18, 19, 23, 24, 26, 30, 31, 39, 43, 92, 93, 135, 136]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_xgb_features = []\n",
    "for i in range(len(clf.feature_importances_)):\n",
    "    if clf.feature_importances_[i] > np.percentile(clf.feature_importances_, 90):\n",
    "        good_xgb_features.append(i)\n",
    "good_xgb_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64667,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_xgb_train = data1[:, -1]\n",
    "y_train_xgb_train[y_train_xgb_train == 2.0] = 0\n",
    "\n",
    "y_train_xgb_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3695"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train_xgb_train)\n",
    "dtest = xgb.DMatrix(X_test1)\n",
    "param = {'max_depth':4, 'eta':0.1, 'silent':1, 'objective':'binary:logistic'}\n",
    "num_round = 10\n",
    "bst = xgb.train(param, dtrain, num_round)\n",
    "# make prediction\n",
    "preds = bst.predict(dtest)\n",
    "np.count_nonzero(preds < 0.58)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
       "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
       "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
       "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=0, silent=True, subsample=1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "# fit model no training data\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhYAAAFkCAYAAAB8RXKEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1854a6be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from xgboost import plot_importance\n",
    "\"\"\"plot_importance(model)\n",
    "plt.show()\"\"\"\n",
    "\n",
    "plt.bar(range(len(model.feature_importances_)), np.sort(model.feature_importances_))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0026178011"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(model.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.777"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "# select features using threshold\n",
    "selection = SelectFromModel(model, threshold=np.mean(model.feature_importances_), prefit = True)\n",
    "select_X_train = selection.transform(X_train)\n",
    "# train model\n",
    "selection_model = XGBClassifier()\n",
    "selection_model.fit(select_X_train, Y_train)\n",
    "# eval model\n",
    "select_X_test = selection.transform(X_test)\n",
    "pred_select = selection_model.predict(select_X_test)\n",
    "xgb_test_accuracy(pred_select, Y_test)/len(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_submission_selectionfeature(leanred_clf, selection_fromtrain):\n",
    "    import csv\n",
    "    with open('test_2008.csv','r') as bv_data2:\n",
    "        data_iter = csv.reader(bv_data2, delimiter = ',', quotechar = '\"') \n",
    "        next(data_iter, None) \n",
    "        data = [data for data in data_iter] \n",
    "    \n",
    "    data_array2 = np.asarray(data,dtype=np.float32)\n",
    "    dim = len(data_array2)\n",
    "    train_data = np.hstack((np.ones(dim).reshape(dim,1),data_array2[:,1:]))  \n",
    "    X_test_out = data_array2[:, :382];\n",
    "    select_X_test_out = selection_fromtrain.transform(X_test_out)\n",
    "    y_test2 = leanred_clf.predict(select_X_test_out)\n",
    "    y_test2[y_test2 == 0] = 2\n",
    "    y_test2 = np.int8(y_test2)\n",
    "    with open('submission2008.csv', 'w', newline='') as csvfile:\n",
    "        spamwriter = csv.writer(csvfile, delimiter=',',\n",
    "                                quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        spamwriter.writerow(['id', 'PES1'])\n",
    "        for i, yi in enumerate(y_test2):\n",
    "            spamwriter.writerow([str(i), str(yi)])\n",
    "            \n",
    "\n",
    "def sample_submission_2008(leanred_clf):\n",
    "    y_test1 = leanred_clf.predict(X_test1)\n",
    "    y_test1 = y_test1/2.0 + 1.5\n",
    "    y_test1 = np.int8(y_test1)\n",
    "    with open('submission2008.csv', 'w', newline='') as csvfile:\n",
    "        spamwriter = csv.writer(csvfile, delimiter=',',\n",
    "                                quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        spamwriter.writerow(['id', 'PES1'])\n",
    "        for i, yi in enumerate(y_test1):\n",
    "            spamwriter.writerow([str(i), str(yi)])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_submission_2008(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "feature_names mismatch: ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31'] ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40', 'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50', 'f51', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f58', 'f59', 'f60', 'f61', 'f62', 'f63', 'f64', 'f65', 'f66', 'f67', 'f68', 'f69', 'f70', 'f71', 'f72', 'f73', 'f74', 'f75', 'f76', 'f77', 'f78', 'f79', 'f80', 'f81', 'f82', 'f83', 'f84', 'f85', 'f86', 'f87', 'f88', 'f89', 'f90', 'f91', 'f92', 'f93', 'f94', 'f95', 'f96', 'f97', 'f98', 'f99', 'f100', 'f101', 'f102', 'f103', 'f104', 'f105', 'f106', 'f107', 'f108', 'f109', 'f110', 'f111', 'f112', 'f113', 'f114', 'f115', 'f116', 'f117', 'f118', 'f119', 'f120', 'f121', 'f122', 'f123', 'f124', 'f125', 'f126', 'f127', 'f128', 'f129', 'f130', 'f131', 'f132', 'f133', 'f134', 'f135', 'f136', 'f137', 'f138', 'f139', 'f140', 'f141', 'f142', 'f143', 'f144', 'f145', 'f146', 'f147', 'f148', 'f149', 'f150', 'f151', 'f152', 'f153', 'f154', 'f155', 'f156', 'f157', 'f158', 'f159', 'f160', 'f161', 'f162', 'f163', 'f164', 'f165', 'f166', 'f167', 'f168', 'f169', 'f170', 'f171', 'f172', 'f173', 'f174', 'f175', 'f176', 'f177', 'f178', 'f179', 'f180', 'f181', 'f182', 'f183', 'f184', 'f185', 'f186', 'f187', 'f188', 'f189', 'f190', 'f191', 'f192', 'f193', 'f194', 'f195', 'f196', 'f197', 'f198', 'f199', 'f200', 'f201', 'f202', 'f203', 'f204', 'f205', 'f206', 'f207', 'f208', 'f209', 'f210', 'f211', 'f212', 'f213', 'f214', 'f215', 'f216', 'f217', 'f218', 'f219', 'f220', 'f221', 'f222', 'f223', 'f224', 'f225', 'f226', 'f227', 'f228', 'f229', 'f230', 'f231', 'f232', 'f233', 'f234', 'f235', 'f236', 'f237', 'f238', 'f239', 'f240', 'f241', 'f242', 'f243', 'f244', 'f245', 'f246', 'f247', 'f248', 'f249', 'f250', 'f251', 'f252', 'f253', 'f254', 'f255', 'f256', 'f257', 'f258', 'f259', 'f260', 'f261', 'f262', 'f263', 'f264', 'f265', 'f266', 'f267', 'f268', 'f269', 'f270', 'f271', 'f272', 'f273', 'f274', 'f275', 'f276', 'f277', 'f278', 'f279', 'f280', 'f281', 'f282', 'f283', 'f284', 'f285', 'f286', 'f287', 'f288', 'f289', 'f290', 'f291', 'f292', 'f293', 'f294', 'f295', 'f296', 'f297', 'f298', 'f299', 'f300', 'f301', 'f302', 'f303', 'f304', 'f305', 'f306', 'f307', 'f308', 'f309', 'f310', 'f311', 'f312', 'f313', 'f314', 'f315', 'f316', 'f317', 'f318', 'f319', 'f320', 'f321', 'f322', 'f323', 'f324', 'f325', 'f326', 'f327', 'f328', 'f329', 'f330', 'f331', 'f332', 'f333', 'f334', 'f335', 'f336', 'f337', 'f338', 'f339', 'f340', 'f341', 'f342', 'f343', 'f344', 'f345', 'f346', 'f347', 'f348', 'f349', 'f350', 'f351', 'f352', 'f353', 'f354', 'f355', 'f356', 'f357', 'f358', 'f359', 'f360', 'f361', 'f362', 'f363', 'f364', 'f365', 'f366', 'f367', 'f368', 'f369', 'f370', 'f371', 'f372', 'f373', 'f374', 'f375', 'f376', 'f377', 'f378', 'f379', 'f380', 'f381']\ntraining data did not have the following fields: f75, f84, f209, f361, f42, f261, f278, f263, f207, f135, f256, f216, f252, f205, f166, f275, f134, f155, f71, f334, f336, f296, f195, f308, f253, f226, f127, f196, f109, f112, f86, f37, f245, f324, f191, f41, f148, f227, f355, f160, f340, f99, f206, f366, f132, f152, f270, f234, f200, f320, f211, f312, f249, f343, f362, f161, f285, f348, f189, f83, f360, f110, f247, f376, f181, f74, f80, f218, f317, f367, f68, f147, f346, f125, f332, f116, f137, f139, f276, f208, f164, f354, f178, f325, f61, f293, f45, f103, f180, f294, f96, f221, f213, f203, f171, f33, f290, f356, f168, f115, f370, f55, f35, f87, f54, f353, f224, f327, f358, f302, f365, f124, f267, f184, f64, f97, f212, f120, f78, f299, f49, f186, f50, f262, f82, f150, f369, f219, f326, f77, f129, f121, f265, f141, f51, f108, f138, f347, f266, f339, f372, f284, f66, f338, f214, f56, f251, f44, f62, f40, f328, f122, f105, f175, f101, f271, f89, f92, f65, f130, f39, f85, f106, f289, f73, f292, f53, f229, f204, f95, f329, f165, f133, f307, f352, f230, f248, f149, f157, f373, f298, f260, f304, f70, f243, f303, f69, f237, f79, f235, f46, f257, f114, f58, f179, f185, f318, f159, f142, f57, f156, f333, f67, f158, f241, f305, f364, f151, f228, f269, f379, f81, f217, f107, f232, f222, f220, f52, f244, f199, f246, f190, f131, f331, f59, f287, f342, f117, f357, f375, f136, f163, f378, f202, f162, f349, f38, f288, f286, f371, f210, f192, f264, f295, f153, f194, f123, f126, f255, f323, f377, f60, f374, f146, f250, f43, f282, f177, f215, f198, f113, f173, f259, f145, f48, f187, f363, f90, f315, f337, f128, f233, f309, f311, f225, f381, f94, f140, f306, f281, f319, f297, f154, f300, f223, f341, f63, f76, f182, f322, f335, f310, f47, f301, f93, f176, f231, f368, f313, f279, f280, f344, f102, f119, f88, f118, f104, f351, f283, f316, f277, f188, f314, f34, f143, f238, f359, f380, f291, f242, f258, f98, f321, f197, f144, f345, f274, f72, f201, f330, f174, f183, f272, f167, f170, f32, f240, f100, f193, f172, f111, f239, f91, f254, f273, f350, f236, f36, f169, f268",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-d92bd7208b5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mX_test_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_array2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m382\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mselect_X_test_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0my_test2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselection_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0my_test2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_test2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0my_test2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint8\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/haoqing/xgboost/python-package/xgboost/sklearn.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, output_margin, ntree_limit)\u001b[0m\n\u001b[1;32m    482\u001b[0m         class_probs = self.booster().predict(test_dmatrix,\n\u001b[1;32m    483\u001b[0m                                              \u001b[0moutput_margin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_margin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m                                              ntree_limit=ntree_limit)\n\u001b[0m\u001b[1;32m    485\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_probs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             \u001b[0mcolumn_indexes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/haoqing/xgboost/python-package/xgboost/core.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, output_margin, ntree_limit, pred_leaf)\u001b[0m\n\u001b[1;32m    948\u001b[0m             \u001b[0moption_mask\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0;36m0x02\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_ulong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/haoqing/xgboost/python-package/xgboost/core.py\u001b[0m in \u001b[0;36m_validate_features\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m                 raise ValueError(msg.format(self.feature_names,\n\u001b[0;32m-> 1193\u001b[0;31m                                             data.feature_names))\n\u001b[0m\u001b[1;32m   1194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_split_value_histogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_pandas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: feature_names mismatch: ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31'] ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40', 'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50', 'f51', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f58', 'f59', 'f60', 'f61', 'f62', 'f63', 'f64', 'f65', 'f66', 'f67', 'f68', 'f69', 'f70', 'f71', 'f72', 'f73', 'f74', 'f75', 'f76', 'f77', 'f78', 'f79', 'f80', 'f81', 'f82', 'f83', 'f84', 'f85', 'f86', 'f87', 'f88', 'f89', 'f90', 'f91', 'f92', 'f93', 'f94', 'f95', 'f96', 'f97', 'f98', 'f99', 'f100', 'f101', 'f102', 'f103', 'f104', 'f105', 'f106', 'f107', 'f108', 'f109', 'f110', 'f111', 'f112', 'f113', 'f114', 'f115', 'f116', 'f117', 'f118', 'f119', 'f120', 'f121', 'f122', 'f123', 'f124', 'f125', 'f126', 'f127', 'f128', 'f129', 'f130', 'f131', 'f132', 'f133', 'f134', 'f135', 'f136', 'f137', 'f138', 'f139', 'f140', 'f141', 'f142', 'f143', 'f144', 'f145', 'f146', 'f147', 'f148', 'f149', 'f150', 'f151', 'f152', 'f153', 'f154', 'f155', 'f156', 'f157', 'f158', 'f159', 'f160', 'f161', 'f162', 'f163', 'f164', 'f165', 'f166', 'f167', 'f168', 'f169', 'f170', 'f171', 'f172', 'f173', 'f174', 'f175', 'f176', 'f177', 'f178', 'f179', 'f180', 'f181', 'f182', 'f183', 'f184', 'f185', 'f186', 'f187', 'f188', 'f189', 'f190', 'f191', 'f192', 'f193', 'f194', 'f195', 'f196', 'f197', 'f198', 'f199', 'f200', 'f201', 'f202', 'f203', 'f204', 'f205', 'f206', 'f207', 'f208', 'f209', 'f210', 'f211', 'f212', 'f213', 'f214', 'f215', 'f216', 'f217', 'f218', 'f219', 'f220', 'f221', 'f222', 'f223', 'f224', 'f225', 'f226', 'f227', 'f228', 'f229', 'f230', 'f231', 'f232', 'f233', 'f234', 'f235', 'f236', 'f237', 'f238', 'f239', 'f240', 'f241', 'f242', 'f243', 'f244', 'f245', 'f246', 'f247', 'f248', 'f249', 'f250', 'f251', 'f252', 'f253', 'f254', 'f255', 'f256', 'f257', 'f258', 'f259', 'f260', 'f261', 'f262', 'f263', 'f264', 'f265', 'f266', 'f267', 'f268', 'f269', 'f270', 'f271', 'f272', 'f273', 'f274', 'f275', 'f276', 'f277', 'f278', 'f279', 'f280', 'f281', 'f282', 'f283', 'f284', 'f285', 'f286', 'f287', 'f288', 'f289', 'f290', 'f291', 'f292', 'f293', 'f294', 'f295', 'f296', 'f297', 'f298', 'f299', 'f300', 'f301', 'f302', 'f303', 'f304', 'f305', 'f306', 'f307', 'f308', 'f309', 'f310', 'f311', 'f312', 'f313', 'f314', 'f315', 'f316', 'f317', 'f318', 'f319', 'f320', 'f321', 'f322', 'f323', 'f324', 'f325', 'f326', 'f327', 'f328', 'f329', 'f330', 'f331', 'f332', 'f333', 'f334', 'f335', 'f336', 'f337', 'f338', 'f339', 'f340', 'f341', 'f342', 'f343', 'f344', 'f345', 'f346', 'f347', 'f348', 'f349', 'f350', 'f351', 'f352', 'f353', 'f354', 'f355', 'f356', 'f357', 'f358', 'f359', 'f360', 'f361', 'f362', 'f363', 'f364', 'f365', 'f366', 'f367', 'f368', 'f369', 'f370', 'f371', 'f372', 'f373', 'f374', 'f375', 'f376', 'f377', 'f378', 'f379', 'f380', 'f381']\ntraining data did not have the following fields: f75, f84, f209, f361, f42, f261, f278, f263, f207, f135, f256, f216, f252, f205, f166, f275, f134, f155, f71, f334, f336, f296, f195, f308, f253, f226, f127, f196, f109, f112, f86, f37, f245, f324, f191, f41, f148, f227, f355, f160, f340, f99, f206, f366, f132, f152, f270, f234, f200, f320, f211, f312, f249, f343, f362, f161, f285, f348, f189, f83, f360, f110, f247, f376, f181, f74, f80, f218, f317, f367, f68, f147, f346, f125, f332, f116, f137, f139, f276, f208, f164, f354, f178, f325, f61, f293, f45, f103, f180, f294, f96, f221, f213, f203, f171, f33, f290, f356, f168, f115, f370, f55, f35, f87, f54, f353, f224, f327, f358, f302, f365, f124, f267, f184, f64, f97, f212, f120, f78, f299, f49, f186, f50, f262, f82, f150, f369, f219, f326, f77, f129, f121, f265, f141, f51, f108, f138, f347, f266, f339, f372, f284, f66, f338, f214, f56, f251, f44, f62, f40, f328, f122, f105, f175, f101, f271, f89, f92, f65, f130, f39, f85, f106, f289, f73, f292, f53, f229, f204, f95, f329, f165, f133, f307, f352, f230, f248, f149, f157, f373, f298, f260, f304, f70, f243, f303, f69, f237, f79, f235, f46, f257, f114, f58, f179, f185, f318, f159, f142, f57, f156, f333, f67, f158, f241, f305, f364, f151, f228, f269, f379, f81, f217, f107, f232, f222, f220, f52, f244, f199, f246, f190, f131, f331, f59, f287, f342, f117, f357, f375, f136, f163, f378, f202, f162, f349, f38, f288, f286, f371, f210, f192, f264, f295, f153, f194, f123, f126, f255, f323, f377, f60, f374, f146, f250, f43, f282, f177, f215, f198, f113, f173, f259, f145, f48, f187, f363, f90, f315, f337, f128, f233, f309, f311, f225, f381, f94, f140, f306, f281, f319, f297, f154, f300, f223, f341, f63, f76, f182, f322, f335, f310, f47, f301, f93, f176, f231, f368, f313, f279, f280, f344, f102, f119, f88, f118, f104, f351, f283, f316, f277, f188, f314, f34, f143, f238, f359, f380, f291, f242, f258, f98, f321, f197, f144, f345, f274, f72, f201, f330, f174, f183, f272, f167, f170, f32, f240, f100, f193, f172, f111, f239, f91, f254, f273, f350, f236, f36, f169, f268"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "with open('test_2008.csv','r') as bv_data2:\n",
    "    data_iter = csv.reader(bv_data2, delimiter = ',', quotechar = '\"') \n",
    "    next(data_iter, None) \n",
    "    data = [data for data in data_iter] \n",
    "\n",
    "data_array2 = np.asarray(data,dtype=np.float32)\n",
    "dim = len(data_array2)\n",
    "train_data = np.hstack((np.ones(dim).reshape(dim,1),data_array2[:,1:])) \n",
    "X_test_out = data_array2[:, :382];\n",
    "select_X_test_out = selection.transform(X_test_out)\n",
    "y_test2 = selection_model.predict(select_X_test_out)\n",
    "y_test2[y_test2 == 0] = 2\n",
    "y_test2 = np.int8(y_test2)\n",
    "with open('submission2008.csv', 'w', newline='') as csvfile:\n",
    "    spamwriter = csv.writer(csvfile, delimiter=',',\n",
    "                            quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "    spamwriter.writerow(['id', 'PES1'])\n",
    "    for i, yi in enumerate(y_test2):\n",
    "        spamwriter.writerow([str(i), str(yi)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16000, 382)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  1., ...,  1.,  1.,  1.])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
